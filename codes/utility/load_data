import numpy as np
import random as rd
import scipy.sparse as sp
from time import time
import json
from utility.parser import parse_args
import pickle
args = parse_args()
import collections

class Data(object):
    def __init__(self, path, batch_size):
        self.path = path + '/%d-core' % args.core
        self.batch_size = args.batch_size

        # get number of users and items
        # Baby dataset
        self.n_users = 19445
        self.n_items = 7050

        # Beauty dataset
        # self.n_users = 22363
        # self.n_items = 12101

        # Toys dataset
        # self.n_users = 19412
        # self.n_items = 11924

        # Phones dataset
        # self.n_users = 27879
        # self.n_items = 10429

        self.n_train, self.n_test = 0, 0

        self.exist_users = []
        self.train_items, self.test_set, self.val_set = {}, {}, {}

        self.train_users = collections.defaultdict(list)
        ############################################################################ 数据集划分
        with open("../data/baby/train.pickle", "rb") as f:
            self.train_items = pickle.load(f)
        with open("../data/baby/test.pickle", "rb") as f:
            self.test_set = pickle.load(f)
        with open("../data/baby/test.pickle", "rb") as f:
            self.val_set = pickle.load(f)

        self.R = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)  # 创建稀疏评分图

        for u_id in range(self.n_users):
            self.exist_users.append(u_id)
            train_items = self.train_items[u_id]
            self.n_train += len(train_items)
            test_items = self.test_set[u_id]
            self.n_test += len(test_items)
            for i_id in train_items:
                self.R[u_id, i_id] = 1.
                self.train_users[i_id].append(u_id)

        self.P_i = list()
        self.C_u = list()

        for i in range(self.n_items):
            train_users = self.train_users[i]
            if train_users == None:
                self.P_i.append(0)
            else:
                self.P_i.append(len(train_users)/self.n_train)

        self.norm_P_i = self.linear_normalize(self.P_i)
        self.P_i_final = np.array(self.norm_P_i)
        # print(len(self.norm_P_i))

        for u_id in range(self.n_users):
            result_list = list()
            train_items = self.train_items[u_id]
            for i_id in train_items:
                train_users = self.train_users[i_id]
                result_list = result_list + train_users
            new_result_list = list(set(result_list))

            if len(new_result_list) == 0:
                self.C_u.append(0)
            else:
                self.C_u.append((len(new_result_list)-1)/self.n_users)
        self.norm_C_u = self.linear_normalize(self.C_u)
        self.C_u_final = np.array(self.norm_C_u)
        # print(len(self.norm_C_u))

    def linear_normalize(self, data):
        # 检查列表是否为空
        if not data:
            return []

            # 找到列表中的最小值和最大值
        min_val = min(data)
        max_val = max(data)

        # 检查最大值和最小值是否相等，避免除以零的错误
        if min_val == max_val:
            return [0.5] * len(data)  # 所有值都相同，归一化为0.5

        # 对列表中的每个值进行归一化
        normalized_data = [(x - min_val) / (max_val - min_val) for x in data]

        return normalized_data

    def get_ii_uu_mat(self):
        i_i_mat = self.R.T.dot(self.R)
        u_u_mat = self.R.dot(self.R.T)
        u_u_mat = u_u_mat.todok()
        i_i_mat = i_i_mat.todok()

        def normalized_adj_double(adj):
            rowsum = np.array(adj.sum(1))

            d_inv = np.power(rowsum, -0.5).flatten()  # 生成D^-1/2度矩阵
            d_inv[np.isinf(d_inv)] = 0.
            d_mat_inv = sp.diags(d_inv)

            norm_adj = d_mat_inv.dot(adj)
            norm_adj = norm_adj.dot(d_mat_inv)
            return norm_adj.tocoo()
        u_u_mat_norm = normalized_adj_double(u_u_mat)
        i_i_mat_norm = normalized_adj_double(i_i_mat)

        return u_u_mat_norm.tocsr(), i_i_mat_norm.tocsr()

    def get_norm_ii_uu_mat(self):
        i_i_mat = self.R.T.dot(self.R)
        u_u_mat = self.R.dot(self.R.T)
        u_u_mat = u_u_mat.tocsr()
        i_i_mat = i_i_mat.tocsr()

        def load_adjacency_list_data(adj_mat):
            tmp = adj_mat.tocoo()
            all_h_list = list(tmp.row)
            all_t_list = list(tmp.col)

            return all_h_list, all_t_list

        all_h_list_uu, all_t_list_uu = load_adjacency_list_data(u_u_mat)
        all_h_list_ii, all_t_list_ii = load_adjacency_list_data(i_i_mat)
        all_v_list_uu = [1] * len(all_h_list_uu)
        all_v_list_ii = [1] * len(all_h_list_ii)

        def create_adj(h_list, t_list, v_list, size):
            data = np.array(v_list)
            row = np.array(h_list)
            col = np.array(t_list)

            # 创建coo_matrix
            sparse_matrix = sp.coo_matrix((data, (row, col)), shape=(size, size))  # shape参数指定了矩阵的大小

            # 转换为CSR格式（如果需要的话），因为CSR格式在很多操作中更高效
            sparse_matrix_dok = sparse_matrix.todok()
            return sparse_matrix_dok

        u_u_mat = create_adj(all_h_list_uu, all_t_list_uu, all_v_list_uu, self.n_users)
        i_i_mat = create_adj(all_h_list_ii, all_t_list_ii, all_v_list_ii, self.n_items)

        def normalized_adj_double(adj):
            rowsum = np.array(adj.sum(1))

            d_inv = np.power(rowsum, -0.5).flatten()  # 生成D^-1/2度矩阵
            d_inv[np.isinf(d_inv)] = 0.
            d_mat_inv = sp.diags(d_inv)

            norm_adj = d_mat_inv.dot(adj)
            norm_adj = norm_adj.dot(d_mat_inv)
            return norm_adj.tocoo()
        u_u_mat_norm = normalized_adj_double(u_u_mat)
        i_i_mat_norm = normalized_adj_double(i_i_mat)

        return u_u_mat_norm.tocsr(), i_i_mat_norm.tocsr()

    def get_adj_mat(self):
        try:
            t1 = time()
            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')
            norm_adj_mat = sp.load_npz(self.path + '/s_norm_adj_mat.npz')
            mean_adj_mat = sp.load_npz(self.path + '/s_mean_adj_mat.npz')
            print('already load adj matrix', adj_mat.shape, time() - t1)

        except Exception:
            adj_mat, norm_adj_mat, mean_adj_mat = self.create_adj_mat()
            # sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)
            # sp.save_npz(self.path + '/s_norm_adj_mat.npz', norm_adj_mat)
            # sp.save_npz(self.path + '/s_mean_adj_mat.npz', mean_adj_mat)
        return adj_mat, norm_adj_mat, mean_adj_mat

    def create_adj_mat(self):
        t1 = time()
        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)
        adj_mat = adj_mat.tolil()
        R = self.R.tolil()

        adj_mat[:self.n_users, self.n_users:] = R
        adj_mat[self.n_users:, :self.n_users] = R.T
        adj_mat = adj_mat.todok()
        print('already create adjacency matrix', adj_mat.shape, time() - t1)

        t2 = time()

        def normalized_adj_single(adj):
            rowsum = np.array(adj.sum(1))

            d_inv = np.power(rowsum, -0.5).flatten()
            d_inv[np.isinf(d_inv)] = 0.
            d_mat_inv = sp.diags(d_inv)

            d_inv_1 = np.power(rowsum, -0.5).flatten()
            d_inv_1[np.isinf(d_inv_1)] = 0.
            d_mat_inv_1 = sp.diags(d_inv_1)

            norm_adj = d_mat_inv.dot(adj)
            norm_adj = norm_adj.dot(d_mat_inv_1)
            # norm_adj = adj.dot(d_mat_inv)
            print('generate single-normalized adjacency matrix.')
            return norm_adj.tocoo()

        def get_D_inv(adj):
            rowsum = np.array(adj.sum(1))

            d_inv = np.power(rowsum, -1).flatten()
            d_inv[np.isinf(d_inv)] = 0.
            d_mat_inv = sp.diags(d_inv)
            return d_mat_inv

        def check_adj_if_equal(adj):
            dense_A = np.array(adj.todense())
            degree = np.sum(dense_A, axis=1, keepdims=False)

            temp = np.dot(np.diag(np.power(degree, -1)), dense_A)
            print('check normalized adjacency matrix whether equal to this laplacian matrix.')
            return temp

        norm_adj_mat = normalized_adj_single(adj_mat + sp.eye(adj_mat.shape[0]))
        mean_adj_mat = normalized_adj_single(adj_mat)

        print('already normalize adjacency matrix', time() - t2)
        return adj_mat.tocsr(), norm_adj_mat.tocsr(), mean_adj_mat.tocsr()


    def sample(self):
        if self.batch_size <= self.n_users:
            users = rd.sample(self.exist_users, self.batch_size)
        else:
            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]
        # users = self.exist_users[:]

        def sample_pos_items_for_u(u, num):
            pos_items = self.train_items[u]
            n_pos_items = len(pos_items)
            pos_batch = []
            while True:
                if len(pos_batch) == num: break
                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]
                pos_i_id = pos_items[pos_id]

                if pos_i_id not in pos_batch:
                    pos_batch.append(pos_i_id)
            return pos_batch

        def sample_neg_items_for_u(u, num):
            neg_items = []
            while True:
                if len(neg_items) == num: break
                neg_id = np.random.randint(low=0, high=self.n_items, size=1)[0]
                if neg_id not in self.train_items[u] and neg_id not in neg_items:
                    neg_items.append(neg_id)
            return neg_items

        def sample_neg_items_for_u_from_pools(u, num):
            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))
            return rd.sample(neg_items, num)

        pos_items, neg_items = [], []
        for u in users:
            pos_items += sample_pos_items_for_u(u, 1)
            neg_items += sample_neg_items_for_u(u, 1)
            # neg_items += sample_neg_items_for_u(u, 3)
        return users, pos_items, neg_items

    def print_statistics(self):
        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))
        print('n_interactions=%d' % (self.n_train + self.n_test))
        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))

