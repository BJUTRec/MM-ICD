import datetime
import math
import os
import random
import sys
from time import time
from tqdm import tqdm

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.sparse as sparse

from utility.parser import parse_args
from Models_MyCode import LGMRec
from utility.batch_test_LGMRec import *

args = parse_args()


class Trainer(object):
    def __init__(self, data_config):
        # argument settings
        self.n_users = data_config['n_users']
        self.n_items = data_config['n_items']

        self.model_name = args.model_name
        self.mess_dropout = eval(args.mess_dropout)
        self.lr = args.lr
        self.emb_dim = args.embed_size
        self.batch_size = args.batch_size
        self.weight_size = eval(args.weight_size)
        self.n_layers = len(self.weight_size)
        self.regs = eval(args.regs)
        self.decay = self.regs[0]

        self.norm_adj = data_config['norm_adj']
        self.norm_adj = self.sparse_mx_to_torch_sparse_tensor(self.norm_adj).float().cuda()

        self.R = data_config['R']
        self.uu = data_config['uu']
        self.ii = data_config['ii']
        self.uu_norm = data_config['uu_norm']
        self.ii_norm = data_config['ii_norm']

        image_feats = np.load('../data/{}/image_feat.npy'.format(args.dataset))
        text_feats = np.load('../data/{}/text_feat.npy'.format(args.dataset))

        self.model = LGMRec(self.n_users, self.n_items, self.emb_dim, self.weight_size, self.mess_dropout, image_feats,
                             text_feats, self.R, self.uu, self.ii, self.uu_norm, self.ii_norm)
        self.model = self.model.cuda()
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        # self.optimizer_1 = optim.Adam(self.model.parameters(), lr=self.lr/10)
        self.lr_scheduler = self.set_lr_scheduler()

    def set_lr_scheduler(self):
        fac = lambda epoch: 0.96 ** (epoch / 50)
        scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=fac)
        return scheduler

    def test(self, users_to_test, is_val):
        self.model.eval()
        with torch.no_grad():
            ua_embeddings, ia_embeddings, _, _ = self.model()
        result = test_torch(ua_embeddings, ia_embeddings, users_to_test, is_val)
        return result

    def train(self):
        training_time_list = []
        loss_loger, pre_loger, rec_loger, ndcg_loger, hit_loger = [], [], [], [], []
        stopping_step = 0
        should_stop = False
        cur_best_pre_0 = 0.

        n_batch = data_generator.n_train // args.batch_size + 1
        best_recall = 0
        for epoch in (range(args.epoch)):
            t1 = time()
            loss, mf_loss, emb_loss = 0., 0., 0.
            n_batch = data_generator.n_train // args.batch_size + 1
            f_time, b_time, loss_time, opt_time, clip_time, emb_time = 0., 0., 0., 0., 0., 0.
            sample_time = 0.

            for idx in tqdm(range(n_batch)):
                self.model.train()
                self.optimizer.zero_grad()
                sample_t1 = time()
                users, pos_items, neg_items = data_generator.sample()
                sample_time += time() - sample_t1
                ua_embeddings, ia_embeddings, hyper_embeddings, ego_embeddings = self.model()
                [uv_embs, iv_embs, ut_embs, it_embs] = hyper_embeddings
                [uv_embs_1, iv_embs_1, ut_embs_1, it_embs_1] = ego_embeddings

                u_g_embeddings_id = ua_embeddings[users]
                pos_i_g_embeddings_id = ia_embeddings[pos_items]
                neg_i_g_embeddings_id = ia_embeddings[neg_items]

                batch_mf_loss_id = self.model.bpr_loss(u_g_embeddings_id,pos_i_g_embeddings_id,neg_i_g_embeddings_id)
                batch_emb_loss_id = self.model.reg_loss(u_g_embeddings_id, pos_i_g_embeddings_id, neg_i_g_embeddings_id)
                batch_ssm_loss_id = self.model.ssm_loss(uv_embs[users], iv_embs[pos_items]) + self.model.ssm_loss(ut_embs[users], it_embs[pos_items])
                batch_ssm_loss_id_1 = self.model.ssm_loss(uv_embs_1[users], iv_embs_1[pos_items]) + self.model.ssm_loss(ut_embs_1[users], it_embs_1[pos_items])

                batch_loss = batch_mf_loss_id + batch_emb_loss_id + batch_ssm_loss_id + batch_ssm_loss_id_1

                batch_loss.backward(retain_graph=True)
                self.optimizer.step()

                loss += float(batch_loss)
                mf_loss += float(batch_mf_loss_id)
                emb_loss += float(batch_emb_loss_id)

            del ua_embeddings, ia_embeddings, u_g_embeddings_id, neg_i_g_embeddings_id, pos_i_g_embeddings_id

            if math.isnan(loss) == True:
                print('ERROR: loss is nan.')
                sys.exit()

            perf_str = 'Epoch %d [%.1fs]: train==[%.5f=%.5f + %.5f]' % (
                epoch, time() - t1, loss, mf_loss, emb_loss)
            training_time_list.append(time() - t1)
            print(perf_str)

            if epoch % args.verbose != 0:
                continue

            t2 = time()
            users_to_test = list(data_generator.test_set.keys())
            users_to_val = list(data_generator.val_set.keys())
            ret = self.test(users_to_val, is_val=True)
            training_time_list.append(t2 - t1)

            t3 = time()

            loss_loger.append(loss)
            rec_loger.append(ret['recall'])
            pre_loger.append(ret['precision'])
            ndcg_loger.append(ret['ndcg'])
            hit_loger.append(ret['hit_ratio'])
            if args.verbose > 0:
                perf_str = 'Epoch %d [%.1fs + %.1fs]:  val==[%.5f=%.5f + %.5f], recall=[%.5f, %.5f], ' \
                           'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f]' % \
                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, ret['recall'][0],
                            ret['recall'][-1],
                            ret['precision'][0], ret['precision'][-1], ret['hit_ratio'][0], ret['hit_ratio'][-1],
                            ret['ndcg'][0], ret['ndcg'][-1])
                print(perf_str)

            if ret['recall'][1] > best_recall:
                best_recall = ret['recall'][1]
                test_ret = self.test(users_to_test, is_val=False)
                perf_str = 'Epoch %d [%.1fs + %.1fs]: test==[%.5f=%.5f + %.5f], recall=[%.5f, %.5f], ' \
                           'precision=[%.5f, %.5f], hit=[%.5f, %.5f], ndcg=[%.5f, %.5f]' % \
                           (epoch, t2 - t1, t3 - t2, loss, mf_loss, emb_loss, test_ret['recall'][0],
                            test_ret['recall'][-1],
                            test_ret['precision'][0], test_ret['precision'][-1], test_ret['hit_ratio'][0],
                            test_ret['hit_ratio'][-1],
                            test_ret['ndcg'][0], test_ret['ndcg'][-1])
                print(perf_str)
                stopping_step = 0

                # user_id_embeddings, item_id_embeddings, user_personal_embeddings, item_personal_embeddings, \
                # user_common_embeddings, item_common_embeddings = self.model.get_embeddings()
                #
                # user_id_embeddings = user_id_embeddings.detach().cpu().numpy()
                # item_id_embeddings = item_id_embeddings.detach().cpu().numpy()
                # user_personal_embeddings = user_personal_embeddings.detach().cpu().numpy()
                # item_personal_embeddings = item_personal_embeddings.detach().cpu().numpy()
                # user_common_embeddings = user_common_embeddings.detach().cpu().numpy()
                # item_common_embeddings = item_common_embeddings.detach().cpu().numpy()
                #
                # np.save('../data/beauty/beauty_user_id_embeddings', user_id_embeddings)
                # np.save('../data/beauty/beauty_item_id_embeddings', item_id_embeddings)
                # np.save('../data/beauty/beauty_user_personal_embeddings', user_personal_embeddings)
                # np.save('../data/beauty/beauty_item_personal_embeddings', item_personal_embeddings)
                # np.save('../data/beauty/beauty_user_common_embeddings', user_common_embeddings)
                # np.save('../data/beauty/beauty_item_common_embeddings', item_common_embeddings)

                # print('Save Successful!!')



            elif stopping_step < args.early_stopping_patience:
                stopping_step += 1
                print('#####Early stopping steps: %d #####' % stopping_step)
            else:
                print('#####Early stop! #####')
                break

        print(test_ret)

    def sparse_mx_to_torch_sparse_tensor(self, sparse_mx):
        """Convert a scipy sparse matrix to a torch sparse tensor."""
        sparse_mx = sparse_mx.tocoo().astype(np.float32)
        indices = torch.from_numpy(
            np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))
        values = torch.from_numpy(sparse_mx.data)
        shape = torch.Size(sparse_mx.shape)
        return torch.sparse.FloatTensor(indices, values, shape)


def set_seed(seed):
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)  # cpu
    torch.cuda.manual_seed_all(seed)  # gpu


if __name__ == '__main__':
    set_seed(args.seed)
    os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu_id)

    config = dict()
    config['n_users'] = data_generator.n_users
    config['n_items'] = data_generator.n_items

    plain_adj, norm_adj, mean_adj = data_generator.get_adj_mat()
    config['norm_adj'] = mean_adj

    rating_matrix = data_generator.R
    config['R'] = rating_matrix

    uu, ii = data_generator.get_ii_uu_mat()
    config['uu'] = uu
    config['ii'] = ii

    uu_norm, ii_norm = data_generator.get_norm_ii_uu_mat()
    config['uu_norm'] = uu_norm
    config['ii_norm'] = ii_norm

    trainer = Trainer(data_config=config)
    trainer.train()

